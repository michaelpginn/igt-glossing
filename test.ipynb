{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[2246, 1, 2729, 1, 3334, 3024, 1, 2601, 1210]"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "\n",
    "from datasets import DatasetDict\n",
    "from src.data import prepare_dataset, create_vocab, create_gloss_vocab\n",
    "from src.encoder import CustomEncoder\n",
    "\n",
    "\n",
    "from src.data import load_data_file\n",
    "\n",
    "train = load_data_file('./data/usp-train-track2-uncovered')\n",
    "dev = load_data_file('./data/usp-dev-track2-uncovered')\n",
    "\n",
    "train_vocab = create_vocab([line.morphemes() for line in train], threshold=1)\n",
    "glosses = create_gloss_vocab()\n",
    "encoder = CustomEncoder(vocabulary=train_vocab, output_vocabulary=glosses)\n",
    "encoder.encode(train[0].morphemes(), vocab='input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "['NEG',\n '[SEP]',\n 'ADJ',\n '[SEP]',\n 'INC',\n 'E3',\n 'VT',\n '[SEP]',\n 'AFI',\n '[SEP]',\n 'DEM']"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0].gloss_list(segmented=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"michaelginn/uspanteko-masked-lm\", use_auth_token=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model.base_model.save_pretrained('./uspanteko-roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(3522, 100, padding_idx=2)\n    (position_embeddings): Embedding(64, 100, padding_idx=2)\n    (token_type_embeddings): Embedding(2, 100)\n    (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=100, out_features=100, bias=True)\n            (key): Linear(in_features=100, out_features=100, bias=True)\n            (value): Linear(in_features=100, out_features=100, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=100, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=100, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=100, bias=True)\n          (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=100, out_features=100, bias=True)\n            (key): Linear(in_features=100, out_features=100, bias=True)\n            (value): Linear(in_features=100, out_features=100, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=100, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=100, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=100, bias=True)\n          (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=100, out_features=100, bias=True)\n            (key): Linear(in_features=100, out_features=100, bias=True)\n            (value): Linear(in_features=100, out_features=100, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=100, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=100, out_features=3072, bias=True)\n          (intermediate_act_fn): GELUActivation()\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=100, bias=True)\n          (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "Downloading config.json:   0%|          | 0.00/681 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c866eef5208046f0aa457852967b11b5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Downloading pytorch_model.bin:   0%|          | 0.00/8.92M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2466fe1da1da4204ac6fa08430b2a7ad"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at michaelginn/uspanteko-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "flat = AutoModelForTokenClassification.from_pretrained(\"michaelginn/uspanteko-roberta-base\", num_labels=66)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaForTokenClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(3522, 100, padding_idx=2)\n      (position_embeddings): Embedding(64, 100, padding_idx=2)\n      (token_type_embeddings): Embedding(2, 100)\n      (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=100, out_features=100, bias=True)\n              (key): Linear(in_features=100, out_features=100, bias=True)\n              (value): Linear(in_features=100, out_features=100, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=100, out_features=100, bias=True)\n              (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=100, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=100, out_features=100, bias=True)\n              (key): Linear(in_features=100, out_features=100, bias=True)\n              (value): Linear(in_features=100, out_features=100, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=100, out_features=100, bias=True)\n              (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=100, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=100, out_features=100, bias=True)\n              (key): Linear(in_features=100, out_features=100, bias=True)\n              (value): Linear(in_features=100, out_features=100, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=100, out_features=100, bias=True)\n              (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=100, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=100, bias=True)\n            (LayerNorm): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=100, out_features=66, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/9774 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bba0be518e2849238fd86c2bc45a21d3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from src.data import prepare_dataset\n",
    "dataset = prepare_dataset(data=data, encoder=encoder,\n",
    "                                           model_input_length=512, mask_tokens_proportion=0.1, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': 'o sey xtok rixoqiil', 'translation': 'O sea busca esposa.', 'glosses': ['CONJ', '[SEP]', 'ADV', '[SEP]', 'COM', 'VT', '[SEP]', 'E3S', 'S'], 'segmentation': \"o' sea x-tok r-ix贸qiil\", 'morphemes': [\"o'\", '[SEP]', 'sea', '[SEP]', 'x', 'tok', '[SEP]', 'r', 'ix贸qiil'], 'input_ids': [0, 0, 0, 1, 0, 3030, 0, 0, 1216, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [61, 1, 54, 1, 25, 39, 1, 14, 43, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "2"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.PAD_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model...\n",
      "1 []\n",
      "before group [0. 0. 0. 0. 0.] 2\n",
      "before group [0. 0. 0. 0. 0.] 2\n",
      "before group [0. 0. 0. 0. 0.] 2\n",
      "before group [0. 0. 0. 0. 0.] 2\n",
      "2 [2.0, 2.0, 2.0, 2.0]\n",
      "3 [2.0, 2.0, 2.0, 2.0]\n",
      "before group [0. 0. 0. 0. 0.] 4\n",
      "4 [2.0, 2.0, 2.0, 3.0]\n",
      "5 [2.0, 2.0, 2.0, 3.0]\n",
      "before group [0. 0. 0. 0. 0.] 6\n",
      "before group [0. 0. 0. 0. 0.] 6\n",
      "6 [2.0, 2.0, 3.0, 4.0]\n",
      "7 [2.0, 2.0, 3.0, 4.0]\n",
      "8 [2.0, 2.0, 3.0, 4.0]\n",
      "before group [0. 0. 0. 0. 0.] 9\n",
      "9 [2.0, 2.0, 3.0, 5.0]\n",
      "10 [2.0, 2.0, 3.0, 5.0]\n",
      "11 [2.0, 2.0, 3.0, 5.0]\n",
      "before group [0. 0. 0. 0. 0.] 12\n",
      "12 [2.0, 2.0, 3.0, 6.0]\n",
      "13 [2.0, 2.0, 3.0, 6.0]\n",
      "14 [2.0, 2.0, 3.0, 6.0]\n",
      "before group [0. 0. 0. 0. 0.] 15\n",
      "15 [2.0, 3.0]\n",
      "16 [2.0, 3.0]\n",
      "17 [2.0, 3.0]\n",
      "before group [0. 0. 0. 0. 0.] 18\n",
      "18 [2.0, 4.0]\n",
      "19 [2.0, 4.0]\n",
      "20 [2.0, 4.0]\n",
      "21 [2.0, 4.0]\n",
      "22 [2.0, 4.0]\n",
      "23 [2.0, 4.0]\n",
      "24 [2.0, 4.0]\n",
      "before group [0. 0. 0. 0. 0.] 25\n",
      "before group [0. 0. 0. 0. 0.] 25\n",
      "25 [2.0, 5.0, 14.0]\n",
      "26 [2.0, 5.0, 14.0]\n",
      "27 [2.0, 5.0, 14.0]\n",
      "28 [2.0, 5.0, 14.0]\n",
      "before group [0. 0. 0. 0. 0.] 29\n",
      "29 [2.0, 5.0, 15.0]\n",
      "30 [2.0, 5.0, 15.0]\n",
      "31 [2.0, 5.0]\n",
      "before group [0. 0. 0. 0. 0.] 32\n",
      "32 [2.0, 6.0]\n",
      "33 [2.0, 6.0]\n",
      "34 [2.0, 6.0]\n",
      "35 [2.0, 6.0]\n",
      "before group [0. 0. 0. 0. 0.] 36\n",
      "36 [2.0, 7.0]\n",
      "37 [2.0, 7.0]\n",
      "38 [2.0, 7.0]\n",
      "39 [2.0, 7.0]\n",
      "before group [0. 0. 0. 0. 0.] 40\n",
      "before group [0. 0. 0. 0. 0.] 40\n",
      "40 [3.0, 8.0]\n",
      "41 [3.0, 8.0]\n",
      "42 [3.0, 8.0]\n",
      "43 [3.0, 8.0]\n",
      "44 [3.0, 8.0]\n",
      "45 [3.0, 8.0]\n",
      "46 [3.0, 8.0]\n",
      "before group [0. 0. 0. 0. 0.] 47\n",
      "47 [3.0, 9.0]\n",
      "48 [3.0, 9.0]\n",
      "49 [3.0, 9.0]\n",
      "50 [3.0, 9.0]\n",
      "before group [0. 0. 0. 0. 0.] 51\n",
      "51 [3.0, 10.0]\n",
      "52 [3.0, 10.0]\n",
      "53 [3.0]\n",
      "54 []\n",
      "before group [0. 0. 0. 0. 0.] 55\n",
      "55 [5.0]\n",
      "56 [5.0]\n",
      "57 [5.0]\n",
      "58 [5.0]\n",
      "before group [0. 0. 0. 0. 0.] 59\n",
      "59 [6.0]\n",
      "60 [6.0]\n",
      "61 []\n",
      "before group [0. 0. 0. 0. 0.] 62\n",
      "62 [8.0]\n",
      "63 [8.0]\n",
      "64 [8.0]\n",
      "65 []\n",
      "BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 3588\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from src.taxonomic_loss_model import create_model\n",
    "from transformers import BertConfig\n",
    "from src.uspanteko_morphology import morphology\n",
    "\n",
    "config = BertConfig(\n",
    "        vocab_size=encoder.vocab_size(),\n",
    "        max_position_embeddings=512,\n",
    "        pad_token_id=encoder.PAD_ID,\n",
    "        num_labels=len(encoder.vocabularies[1])\n",
    "    )\n",
    "model = create_model(encoder, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 66])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# o = model.forward(input_ids=torch.LongTensor(dataset[:2]['input_ids']),\n",
    "#               attention_mask=torch.LongTensor(dataset[:2]['attention_mask']))\n",
    "o = model.forward(input_ids=torch.LongTensor(dataset[:2]['input_ids']),\n",
    "              attention_mask=torch.LongTensor(dataset[:2]['attention_mask']),\n",
    "                  labels=torch.LongTensor(dataset[:2]['labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "o.logits.view(-1, 66).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(20.3891, grad_fn=<AddBackward0>)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "o.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'JupyterNotebookCallback' object has no attribute 'on_init_end'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [54], line 87\u001B[0m\n\u001B[1;32m     75\u001B[0m jupyter_callback \u001B[38;5;241m=\u001B[39m JupyterNotebookCallback()\n\u001B[1;32m     77\u001B[0m args \u001B[38;5;241m=\u001B[39m TrainingArguments(\n\u001B[1;32m     78\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../training-checkpoints\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     79\u001B[0m     num_train_epochs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     84\u001B[0m     logging_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[1;32m     85\u001B[0m )\n\u001B[0;32m---> 87\u001B[0m trainer \u001B[38;5;241m=\u001B[39m \u001B[43mTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     88\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     89\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     90\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     91\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     92\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     93\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     94\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mjupyter_callback\u001B[49m\u001B[43m]\u001B[49m\n\u001B[1;32m     95\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer.py:624\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m    622\u001B[0m default_label_names \u001B[38;5;241m=\u001B[39m find_labels(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m)\n\u001B[1;32m    623\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabel_names \u001B[38;5;241m=\u001B[39m default_label_names \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mlabel_names \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mlabel_names\n\u001B[0;32m--> 624\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcontrol \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcallback_handler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_init_end\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    626\u001B[0m \u001B[38;5;66;03m# Internal variables to keep track of the original batch size\u001B[39;00m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_train_batch_size \u001B[38;5;241m=\u001B[39m args\u001B[38;5;241m.\u001B[39mtrain_batch_size\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer_callback.py:349\u001B[0m, in \u001B[0;36mCallbackHandler.on_init_end\u001B[0;34m(self, args, state, control)\u001B[0m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_init_end\u001B[39m(\u001B[38;5;28mself\u001B[39m, args: TrainingArguments, state: TrainerState, control: TrainerControl):\n\u001B[0;32m--> 349\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall_event\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mon_init_end\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontrol\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniforge3/lib/python3.10/site-packages/transformers/trainer_callback.py:397\u001B[0m, in \u001B[0;36mCallbackHandler.call_event\u001B[0;34m(self, event, args, state, control, **kwargs)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcall_event\u001B[39m(\u001B[38;5;28mself\u001B[39m, event, args, state, control, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    396\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m--> 397\u001B[0m         result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mgetattr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m)\u001B[49m(\n\u001B[1;32m    398\u001B[0m             args,\n\u001B[1;32m    399\u001B[0m             state,\n\u001B[1;32m    400\u001B[0m             control,\n\u001B[1;32m    401\u001B[0m             model\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel,\n\u001B[1;32m    402\u001B[0m             tokenizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer,\n\u001B[1;32m    403\u001B[0m             optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptimizer,\n\u001B[1;32m    404\u001B[0m             lr_scheduler\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlr_scheduler,\n\u001B[1;32m    405\u001B[0m             train_dataloader\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_dataloader,\n\u001B[1;32m    406\u001B[0m             eval_dataloader\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meval_dataloader,\n\u001B[1;32m    407\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    408\u001B[0m         )\n\u001B[1;32m    409\u001B[0m         \u001B[38;5;66;03m# A Callback can skip the return of `control` if it doesn't change it.\u001B[39;00m\n\u001B[1;32m    410\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m result \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'JupyterNotebookCallback' object has no attribute 'on_init_end'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "\n",
    "def eval_accuracy(pred, gold) -> dict:\n",
    "    \"\"\"Computes the average and overall accuracy, where predicted labels must be in the correct position in the list.\"\"\"\n",
    "    total_correct_predictions = 0\n",
    "    total_tokens = 0\n",
    "    summed_accuracies = 0\n",
    "\n",
    "    for (entry_pred, entry_gold, i) in zip(pred, gold, range(len(gold))):\n",
    "        entry_correct_predictions = 0\n",
    "\n",
    "        for token_index in range(len(entry_gold)):\n",
    "            # For each token, check if it matches\n",
    "            if token_index < len(entry_pred) and \\\n",
    "                    entry_pred[token_index] == entry_gold[token_index] and \\\n",
    "                    entry_pred[token_index] not in ['[UNK]', '[SEP]']:\n",
    "                entry_correct_predictions += 1\n",
    "\n",
    "        entry_accuracy = (entry_correct_predictions / len(entry_gold))\n",
    "        summed_accuracies += entry_accuracy\n",
    "\n",
    "        total_correct_predictions += entry_correct_predictions\n",
    "        total_tokens += len([token for token in entry_gold if token != '[SEP]'])\n",
    "\n",
    "    total_entries = len(gold)\n",
    "    average_accuracy = summed_accuracies / total_entries\n",
    "    overall_accuracy = total_correct_predictions / total_tokens\n",
    "    return {'average_accuracy': average_accuracy, 'accuracy': overall_accuracy}\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    # Decode predicted output\n",
    "    print(preds)\n",
    "    decoded_preds = encoder.batch_decode(preds, from_vocabulary_index=1)\n",
    "    print(decoded_preds[0:1])\n",
    "\n",
    "    # Decode (gold) labels\n",
    "    print(labels)\n",
    "    labels = np.where(labels != -100, labels, encoder.PAD_ID)\n",
    "    decoded_labels = encoder.batch_decode(labels, from_vocabulary_index=1)\n",
    "    print(decoded_labels[0:1])\n",
    "\n",
    "    return eval_accuracy(decoded_preds, decoded_labels)\n",
    "\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    return logits.argmax(dim=2)\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=1, # set batch size to 1\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 1\n",
      "  Batch size = 8\n",
      "The following columns in the test set don't have a corresponding argument in `HierarchicalMorphemeLabelingModel.forward` and have been ignored: glosses, morphemes, segmentation, translation, transcription. If glosses, morphemes, segmentation, translation, transcription are not expected by `HierarchicalMorphemeLabelingModel.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1/1 : < :]\n    </div>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "PredictionOutput(predictions=array([[[3.0975988e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7604112e-14],\n        [3.0975927e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7604112e-14],\n        [3.0975927e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7604112e-14],\n        ...,\n        [3.0975870e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7603983e-14],\n        [3.0975927e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7604112e-14],\n        [3.0975870e-14, 1.0000000e+00, 6.6980982e-01, ...,\n         5.4001689e-01, 1.3051839e-01, 6.7603854e-14]]], dtype=float32), label_ids=array([[  61,    1,   54,    1,   25,   39,    1,   14,   43, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n        -100, -100, -100, -100, -100, -100]]), metrics={'test_loss': 4.033997058868408, 'test_runtime': 0.2487, 'test_samples_per_second': 4.022, 'test_steps_per_second': 4.022})"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.predict(test_dataset=[dataset[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "{'transcription': 'o sey xtok rixoqiil',\n 'translation': 'O sea busca esposa.',\n 'glosses': ['CONJ',\n  '[SEP]',\n  'ADV',\n  '[SEP]',\n  'COM',\n  'VT',\n  '[SEP]',\n  'E3S',\n  'S'],\n 'segmentation': \"o' sea x-tok r-ix贸qiil\",\n 'morphemes': [\"o'\",\n  '[SEP]',\n  'sea',\n  '[SEP]',\n  'x',\n  'tok',\n  '[SEP]',\n  'r',\n  'ix贸qiil'],\n 'input_ids': [2252,\n  1,\n  2735,\n  1,\n  3340,\n  3030,\n  1,\n  2607,\n  1216,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2,\n  2],\n 'attention_mask': [1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  1,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0,\n  0],\n 'labels': [61,\n  1,\n  54,\n  1,\n  25,\n  39,\n  1,\n  14,\n  43,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100,\n  -100]}"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "data": {
      "text/plain": "      0     1     2     3     4\n0   0.0   0.0   0.0   0.0   0.0\n1   1.0   1.0   1.0   1.0   1.0\n2   2.0   2.0   2.0   2.0   2.0\n3   2.0   2.0   2.0   2.0   3.0\n4   2.0   2.0   2.0   3.0   4.0\n..  ...   ...   ...   ...   ...\n61  7.0  19.0  46.0  53.0  61.0\n62  8.0  20.0  47.0  54.0  62.0\n63  8.0  21.0  48.0  55.0  63.0\n64  8.0  22.0  49.0  56.0  64.0\n65  9.0  23.0  50.0  57.0  65.0\n\n[66 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>2.0</td>\n      <td>3.0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>7.0</td>\n      <td>19.0</td>\n      <td>46.0</td>\n      <td>53.0</td>\n      <td>61.0</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>8.0</td>\n      <td>20.0</td>\n      <td>47.0</td>\n      <td>54.0</td>\n      <td>62.0</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>8.0</td>\n      <td>21.0</td>\n      <td>48.0</td>\n      <td>55.0</td>\n      <td>63.0</td>\n    </tr>\n    <tr>\n      <th>64</th>\n      <td>8.0</td>\n      <td>22.0</td>\n      <td>49.0</td>\n      <td>56.0</td>\n      <td>64.0</td>\n    </tr>\n    <tr>\n      <th>65</th>\n      <td>9.0</td>\n      <td>23.0</td>\n      <td>50.0</td>\n      <td>57.0</td>\n      <td>65.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>66 rows  5 columns</p>\n</div>"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(m)\n",
    "df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 3",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [78], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlogits\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mlist\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroupby\u001B[49m\u001B[43m(\u001B[49m\u001B[43mby\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\n",
      "\u001B[0;31mIndexError\u001B[0m: too many indices for tensor of dimension 3"
     ]
    }
   ],
   "source": [
    "o.logits[list(df.groupby(by=1).groups.values())]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1024, 24])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for group in list(df.groupby(by=1).groups.values()):\n",
    "#     print(o.logits.view(-1, 66)[:,group].sum(axis=1))\n",
    "\n",
    "torch.transpose(torch.stack([o.logits.view(-1, 66)[:,group].sum(axis=1) for group in list(df.groupby(by=1).groups.values())]), 0, 1).size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TokenClassifierOutput' object has no attribute 'labels'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [97], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mo\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlabels\u001B[49m\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'TokenClassifierOutput' object has no attribute 'labels'"
     ]
    }
   ],
   "source": [
    "o.labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [
    {
     "data": {
      "text/plain": "(1024,)"
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = torch.LongTensor(dataset[:2]['labels']).view(-1)\n",
    "labels[labels == -100] = 66\n",
    "np.vstack([df, [-100] * 5])[labels, 1].shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape[1]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "def get_hierarchy_matrix(self, hierarchy_tree, num_tags, max_depth):\n",
    "    \"\"\"Takes a hierarchical tree, and creates a matrix of a_i,j where i is the tag and j is the level of hierarchy.\n",
    "    \"\"\"\n",
    "    matrix = np.zeros((num_tags, max_depth))\n",
    "\n",
    "    def parse_tree(tree, group_prefix=[], start_tag=0):\n",
    "        for group_index, item in enumerate(tree):\n",
    "            if start_tag == 0:\n",
    "                start_tag += 1\n",
    "                continue\n",
    "            if isinstance(item, tuple):\n",
    "                start_tag = parse_tree(item[1],\n",
    "                                       group_prefix=group_prefix + [matrix[start_tag - 1][len(group_prefix)] + 1],\n",
    "                                       start_tag=start_tag)\n",
    "            else:\n",
    "                matrix[start_tag] = matrix[start_tag - 1] + 1\n",
    "                matrix[start_tag][:len(group_prefix)] = group_prefix\n",
    "                start_tag += 1\n",
    "        return start_tag\n",
    "\n",
    "    parse_tree(hierarchy_tree)\n",
    "    return matrix\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 1]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "torch.LongTensor(np.ones((1,2)), device='cpu')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from src.uspanteko_morphology import morphology\n",
    "\n",
    "def create_gloss_vocab():\n",
    "    def parse_tree(morphology_subtree):\n",
    "        all_glosses = []\n",
    "        for item in morphology_subtree:\n",
    "            if isinstance(item, tuple):\n",
    "                all_glosses += parse_tree(item[1])\n",
    "            else:\n",
    "                all_glosses.append(item)\n",
    "        return all_glosses\n",
    "\n",
    "    return parse_tree(morphology)\n",
    "\n",
    "glosses = create_gloss_vocab()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "66"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(glosses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
