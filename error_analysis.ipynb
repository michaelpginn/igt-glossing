{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "761b9f71fedd42faa219686c1aaafb8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9774 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b33590803c3d4199af42e9fa38a5f8e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/232 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./src')\n",
    "\n",
    "from src.data import prepare_dataset, create_vocab, create_gloss_vocab, special_chars, load_data_file\n",
    "import random\n",
    "from src.tokenizer import WordLevelTokenizer\n",
    "from src.uspanteko_morphology import morphology\n",
    "from datasets import DatasetDict\n",
    "\n",
    "# Load data\n",
    "train_data = load_data_file(f\"./data/usp-train-track2-uncovered\")\n",
    "dev_data = load_data_file(f\"./data/usp-dev-track2-uncovered\")\n",
    "\n",
    "train_vocab = create_vocab([line.morphemes() for line in train_data], threshold=1)\n",
    "tokenizer = WordLevelTokenizer(vocab=train_vocab, model_max_length=64)\n",
    "\n",
    "glosses = create_gloss_vocab(morphology)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = prepare_dataset(data=train_data, tokenizer=tokenizer, labels=glosses, device='cpu')\n",
    "dataset['dev'] = prepare_dataset(data=dev_data, tokenizer=tokenizer, labels=glosses, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.26302124311565694, 0.24130605822187254, 0.2...</td>\n",
       "      <td>(0.38874901652242333, 0.3643587726199843, 0.35...</td>\n",
       "      <td>(0.47033831628638867, 0.42446892210857595, 0.4...</td>\n",
       "      <td>(0.5184893784421715, 0.4749016522423289, 0.468...</td>\n",
       "      <td>(0.5533438237608183, 0.5130605822187253, 0.504...</td>\n",
       "      <td>(0.5837922895357986, 0.5429583005507475, 0.536...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.29960660896931557, 0.3286388670338317, 0.34...</td>\n",
       "      <td>(0.46388670338316296, 0.48080251770259635, 0.5...</td>\n",
       "      <td>(0.5402832415420928, 0.552006294256491, 0.5818...</td>\n",
       "      <td>(0.598426435877262, 0.5944138473642802, 0.6394...</td>\n",
       "      <td>(0.6424075531077891, 0.6276947285601888, 0.675...</td>\n",
       "      <td>(0.6745082612116444, 0.6549173878835562, 0.704...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.47285601888276946, 0.45719905586152637, 0.4...</td>\n",
       "      <td>(0.6333595594020458, 0.6586939417781276, 0.635...</td>\n",
       "      <td>(0.7128245476003147, 0.7382376081825335, 0.734...</td>\n",
       "      <td>(0.7664044059795436, 0.7843430369787567, 0.779...</td>\n",
       "      <td>(0.8041699449252556, 0.8167584579071597, 0.814...</td>\n",
       "      <td>(0.8332022029897719, 0.8374508261211645, 0.839...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.6808025177025964, 0.6530291109362707, 0.677...</td>\n",
       "      <td>(0.8243902439024391, 0.8114083398898506, 0.818...</td>\n",
       "      <td>(0.8697088906372933, 0.8595594020456334, 0.866...</td>\n",
       "      <td>(0.8948072383949646, 0.8845003933910307, 0.897...</td>\n",
       "      <td>(0.9101494885916601, 0.9035405192761606, 0.915...</td>\n",
       "      <td>(0.9230527143981118, 0.9201416207710466, 0.928...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0   \n",
       "0  (0.26302124311565694, 0.24130605822187254, 0.2...  \\\n",
       "1  (0.29960660896931557, 0.3286388670338317, 0.34...   \n",
       "2  (0.47285601888276946, 0.45719905586152637, 0.4...   \n",
       "3  (0.6808025177025964, 0.6530291109362707, 0.677...   \n",
       "\n",
       "                                                   1   \n",
       "0  (0.38874901652242333, 0.3643587726199843, 0.35...  \\\n",
       "1  (0.46388670338316296, 0.48080251770259635, 0.5...   \n",
       "2  (0.6333595594020458, 0.6586939417781276, 0.635...   \n",
       "3  (0.8243902439024391, 0.8114083398898506, 0.818...   \n",
       "\n",
       "                                                   2   \n",
       "0  (0.47033831628638867, 0.42446892210857595, 0.4...  \\\n",
       "1  (0.5402832415420928, 0.552006294256491, 0.5818...   \n",
       "2  (0.7128245476003147, 0.7382376081825335, 0.734...   \n",
       "3  (0.8697088906372933, 0.8595594020456334, 0.866...   \n",
       "\n",
       "                                                   3   \n",
       "0  (0.5184893784421715, 0.4749016522423289, 0.468...  \\\n",
       "1  (0.598426435877262, 0.5944138473642802, 0.6394...   \n",
       "2  (0.7664044059795436, 0.7843430369787567, 0.779...   \n",
       "3  (0.8948072383949646, 0.8845003933910307, 0.897...   \n",
       "\n",
       "                                                   4   \n",
       "0  (0.5533438237608183, 0.5130605822187253, 0.504...  \\\n",
       "1  (0.6424075531077891, 0.6276947285601888, 0.675...   \n",
       "2  (0.8041699449252556, 0.8167584579071597, 0.814...   \n",
       "3  (0.9101494885916601, 0.9035405192761606, 0.915...   \n",
       "\n",
       "                                                   5  \n",
       "0  (0.5837922895357986, 0.5429583005507475, 0.536...  \n",
       "1  (0.6745082612116444, 0.6549173878835562, 0.704...  \n",
       "2  (0.8332022029897719, 0.8374508261211645, 0.839...  \n",
       "3  (0.9230527143981118, 0.9201416207710466, 0.928...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForTokenClassification\n",
    "from src.taxonomic_loss_model import TaxonomicLossModel\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, gold_labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "\n",
    "    total_morphemes = 0\n",
    "    total_correct = 0\n",
    "    total_top_5_correct = 0\n",
    "\n",
    "    # Calculate top k accuracy\n",
    "    for seq_index in range(len(gold_labels)):\n",
    "        for token_index in range(len(gold_labels[seq_index])):\n",
    "            correct_token_id = gold_labels[seq_index][token_index]\n",
    "            if len(glosses) > correct_token_id >= 0 and correct_token_id != 1:\n",
    "                total_morphemes += 1\n",
    "                if correct_token_id in preds[seq_index][token_index]:\n",
    "                    total_top_5_correct += 1\n",
    "                if correct_token_id == preds[seq_index][token_index][0]:\n",
    "                    total_correct += 1\n",
    "\n",
    "    return {'accuracy': total_correct / total_morphemes, 'topkaccuracy': total_top_5_correct / total_morphemes}\n",
    "\n",
    "def preprocess_logits_for_metrics(topk):\n",
    "    def _preprocess_logits_for_metrics(logits, labels):\n",
    "        return torch.topk(logits, topk, dim=2).indices\n",
    "    return _preprocess_logits_for_metrics\n",
    "\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=100,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "def eval_topk(size, k):\n",
    "    tax_accs = []\n",
    "    harmonic_accs = []\n",
    "    flat_accs = []\n",
    "\n",
    "    for seed in range(42, 52):\n",
    "        tax_model = TaxonomicLossModel.from_pretrained(f\"./models/{size}-tax-{seed}\", num_labels=len(glosses))\n",
    "        tax_model.use_morphology_tree(morphology, 5)\n",
    "\n",
    "        harmonic_tax_model = TaxonomicLossModel.from_pretrained(f\"./models/{size}-tax-{seed}-harmonic\", num_labels=len(glosses))\n",
    "        harmonic_tax_model.use_morphology_tree(morphology, 5)\n",
    "\n",
    "        flat_model = AutoModelForTokenClassification.from_pretrained(f\"./models/{size}-flat-{seed}\", num_labels=len(glosses))\n",
    "\n",
    "        tax_trainer = Trainer(\n",
    "            tax_model,\n",
    "            args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    "        )\n",
    "\n",
    "        harmonic_trainer = Trainer(\n",
    "            harmonic_tax_model,\n",
    "            args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    "        )\n",
    "\n",
    "        flat_trainer = Trainer(\n",
    "            flat_model,\n",
    "            args,\n",
    "            compute_metrics=compute_metrics,\n",
    "            preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    "        )\n",
    "\n",
    "        flat_accs.append(flat_trainer.evaluate(dataset['dev'])['eval_topkaccuracy'])\n",
    "        tax_accs.append(tax_trainer.evaluate(dataset['dev'])['eval_topkaccuracy'])\n",
    "        harmonic_accs.append(harmonic_trainer.evaluate(dataset['dev'])['eval_topkaccuracy'])\n",
    "\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    return sum(flat_accs) / len(flat_accs), sum(tax_accs) / len(tax_accs), sum(harmonic_accs) / len(harmonic_accs)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for size in [10, 100, 500, 1000]:\n",
    "    size_results = []\n",
    "    for k in [1, 2, 3, 4, 5, 6]:\n",
    "        flat_score, tax_score, harmonic_score = eval_topk(size, k)\n",
    "        size_results.append((flat_score, tax_score, harmonic_score))\n",
    "    all_results.append(size_results)\n",
    "\n",
    "pd.DataFrame(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(0.263, 0.241, 0.248)</td>\n",
       "      <td>(0.389, 0.364, 0.355)</td>\n",
       "      <td>(0.47, 0.424, 0.427)</td>\n",
       "      <td>(0.518, 0.475, 0.468)</td>\n",
       "      <td>(0.553, 0.513, 0.504)</td>\n",
       "      <td>(0.584, 0.543, 0.536)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(0.3, 0.329, 0.348)</td>\n",
       "      <td>(0.464, 0.481, 0.503)</td>\n",
       "      <td>(0.54, 0.552, 0.582)</td>\n",
       "      <td>(0.598, 0.594, 0.639)</td>\n",
       "      <td>(0.642, 0.628, 0.675)</td>\n",
       "      <td>(0.675, 0.655, 0.704)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(0.473, 0.457, 0.431)</td>\n",
       "      <td>(0.633, 0.659, 0.635)</td>\n",
       "      <td>(0.713, 0.738, 0.734)</td>\n",
       "      <td>(0.766, 0.784, 0.779)</td>\n",
       "      <td>(0.804, 0.817, 0.815)</td>\n",
       "      <td>(0.833, 0.837, 0.839)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(0.681, 0.653, 0.677)</td>\n",
       "      <td>(0.824, 0.811, 0.818)</td>\n",
       "      <td>(0.87, 0.86, 0.867)</td>\n",
       "      <td>(0.895, 0.885, 0.898)</td>\n",
       "      <td>(0.91, 0.904, 0.915)</td>\n",
       "      <td>(0.923, 0.92, 0.928)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       0                      1                      2   \n",
       "0  (0.263, 0.241, 0.248)  (0.389, 0.364, 0.355)   (0.47, 0.424, 0.427)  \\\n",
       "1    (0.3, 0.329, 0.348)  (0.464, 0.481, 0.503)   (0.54, 0.552, 0.582)   \n",
       "2  (0.473, 0.457, 0.431)  (0.633, 0.659, 0.635)  (0.713, 0.738, 0.734)   \n",
       "3  (0.681, 0.653, 0.677)  (0.824, 0.811, 0.818)    (0.87, 0.86, 0.867)   \n",
       "\n",
       "                       3                      4                      5  \n",
       "0  (0.518, 0.475, 0.468)  (0.553, 0.513, 0.504)  (0.584, 0.543, 0.536)  \n",
       "1  (0.598, 0.594, 0.639)  (0.642, 0.628, 0.675)  (0.675, 0.655, 0.704)  \n",
       "2  (0.766, 0.784, 0.779)  (0.804, 0.817, 0.815)  (0.833, 0.837, 0.839)  \n",
       "3  (0.895, 0.885, 0.898)   (0.91, 0.904, 0.915)   (0.923, 0.92, 0.928)  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(all_results)\n",
    "df.applymap(lambda x: (round(x[0], 3), round(x[1], 3), round(x[2], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./models/1000-tax-42/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"michaelginn/uspanteko-masked-lm\",\n",
      "  \"architectures\": [\n",
      "    \"TaxonomicLossModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 3520\n",
      "}\n",
      "\n",
      "loading weights file ./models/1000-tax-42/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing TaxonomicLossModel.\n",
      "\n",
      "All the weights of TaxonomicLossModel were initialized from the model checkpoint at ./models/1000-tax-42.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TaxonomicLossModel for predictions without further training.\n",
      "loading configuration file ./models/1000-tax-42-harmonic/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"michaelginn/uspanteko-masked-lm\",\n",
      "  \"architectures\": [\n",
      "    \"TaxonomicLossModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 3520\n",
      "}\n",
      "\n",
      "loading weights file ./models/1000-tax-42-harmonic/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing TaxonomicLossModel.\n",
      "\n",
      "All the weights of TaxonomicLossModel were initialized from the model checkpoint at ./models/1000-tax-42-harmonic.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TaxonomicLossModel for predictions without further training.\n",
      "loading configuration file ./models/1000-flat-42/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"./models/1000-flat-42\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 3520\n",
      "}\n",
      "\n",
      "loading weights file ./models/1000-flat-42/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ./models/1000-flat-42.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: glosses, translation, segmentation, morphemes, transcription. If glosses, translation, segmentation, morphemes, transcription are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 232\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `TaxonomicLossModel.forward` and have been ignored: glosses, translation, segmentation, morphemes, transcription. If glosses, translation, segmentation, morphemes, transcription are not expected by `TaxonomicLossModel.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 232\n",
      "  Batch size = 16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVEL 0 tensor(0.4146)\n",
      "LEVEL 1 tensor(0.6322)\n",
      "LEVEL 2 tensor(1.4254)\n",
      "LEVEL 3 tensor(1.7305)\n",
      "LEVEL 4 tensor(1.9446)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LEVEL 0 tensor(0.3269)\n",
      "LEVEL 1 tensor(0.6287)\n",
      "LEVEL 2 tensor(1.3686)\n",
      "LEVEL 3 tensor(1.6942)\n",
      "LEVEL 4 tensor(1.9460)\n",
      "LEVEL 0 tensor(0.5675)\n",
      "LEVEL 1 tensor(0.7645)\n",
      "LEVEL 2 tensor(1.7684)\n",
      "LEVEL 3 tensor(1.9330)\n",
      "LEVEL 4 tensor(2.1195)\n",
      "LEVEL 0 tensor(0.3524)\n",
      "LEVEL 1 tensor(0.6436)\n",
      "LEVEL 2 tensor(1.5724)\n",
      "LEVEL 3 tensor(1.7886)\n",
      "LEVEL 4 tensor(2.0093)\n",
      "LEVEL 0 tensor(0.3385)\n",
      "LEVEL 1 tensor(0.5933)\n",
      "LEVEL 2 tensor(1.4456)\n",
      "LEVEL 3 tensor(1.7077)\n",
      "LEVEL 4 tensor(1.9664)\n",
      "LEVEL 0 tensor(0.2986)\n",
      "LEVEL 1 tensor(0.5916)\n",
      "LEVEL 2 tensor(1.4352)\n",
      "LEVEL 3 tensor(1.6942)\n",
      "LEVEL 4 tensor(1.8781)\n",
      "LEVEL 0 tensor(0.3214)\n",
      "LEVEL 1 tensor(0.7107)\n",
      "LEVEL 2 tensor(1.4681)\n",
      "LEVEL 3 tensor(1.7258)\n",
      "LEVEL 4 tensor(1.8728)\n",
      "LEVEL 0 tensor(0.4715)\n",
      "LEVEL 1 tensor(0.6778)\n",
      "LEVEL 2 tensor(1.4500)\n",
      "LEVEL 3 tensor(1.7203)\n",
      "LEVEL 4 tensor(1.9083)\n",
      "LEVEL 0 tensor(0.6914)\n",
      "LEVEL 1 tensor(0.8779)\n",
      "LEVEL 2 tensor(1.6297)\n",
      "LEVEL 3 tensor(1.8415)\n",
      "LEVEL 4 tensor(2.0168)\n",
      "LEVEL 0 tensor(0.3581)\n",
      "LEVEL 1 tensor(0.6959)\n",
      "LEVEL 2 tensor(1.7130)\n",
      "LEVEL 3 tensor(1.9223)\n",
      "LEVEL 4 tensor(2.0990)\n",
      "LEVEL 0 tensor(0.5649)\n",
      "LEVEL 1 tensor(0.7742)\n",
      "LEVEL 2 tensor(1.6382)\n",
      "LEVEL 3 tensor(1.8805)\n",
      "LEVEL 4 tensor(2.1401)\n",
      "LEVEL 0 tensor(0.5258)\n",
      "LEVEL 1 tensor(0.7513)\n",
      "LEVEL 2 tensor(1.4705)\n",
      "LEVEL 3 tensor(1.7351)\n",
      "LEVEL 4 tensor(1.9227)\n",
      "LEVEL 0 tensor(0.6977)\n",
      "LEVEL 1 tensor(0.8444)\n",
      "LEVEL 2 tensor(1.5759)\n",
      "LEVEL 3 tensor(1.8494)\n",
      "LEVEL 4 tensor(1.9914)\n",
      "LEVEL 0 tensor(0.3859)\n",
      "LEVEL 1 tensor(0.6335)\n",
      "LEVEL 2 tensor(1.7139)\n",
      "LEVEL 3 tensor(1.8826)\n",
      "LEVEL 4 tensor(2.0305)\n",
      "LEVEL 0 tensor(0.4268)\n",
      "LEVEL 1 tensor(0.7972)\n",
      "LEVEL 2 tensor(1.5694)\n",
      "LEVEL 3 tensor(1.7924)\n",
      "LEVEL 4 tensor(1.9539)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Explore some of the wrong predictions made by our model\n",
    "incorrect = []\n",
    "\n",
    "tax_model = TaxonomicLossModel.from_pretrained(f\"./models/1000-tax-42\", num_labels=len(glosses))\n",
    "tax_model.use_morphology_tree(morphology, 5)\n",
    "\n",
    "harmonic_tax_model = TaxonomicLossModel.from_pretrained(f\"./models/1000-tax-42-harmonic\", num_labels=len(glosses))\n",
    "harmonic_tax_model.use_morphology_tree(morphology, 5)\n",
    "\n",
    "flat_model = AutoModelForTokenClassification.from_pretrained(f\"./models/1000-flat-42\", num_labels=len(glosses))\n",
    "        \n",
    "        \n",
    "tax_trainer = Trainer(\n",
    "    tax_model,\n",
    "    args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    ")\n",
    "\n",
    "harmonic_trainer = Trainer(\n",
    "    harmonic_tax_model,\n",
    "    args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    ")\n",
    "\n",
    "flat_trainer = Trainer(\n",
    "    flat_model,\n",
    "    args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics(k),\n",
    ")\n",
    "        \n",
    "flat_preds = flat_trainer.predict(dataset['dev'])\n",
    "tax_preds = tax_trainer.predict(dataset['dev'])\n",
    "\n",
    "for seq_index in range(len(flat_preds.label_ids)):\n",
    "    for token_index in range(len(flat_preds.label_ids[seq_index])):\n",
    "        correct_token_id = flat_preds.label_ids[seq_index][token_index]\n",
    "        if len(glosses) > correct_token_id >= 0 and correct_token_id != 1:\n",
    "            if correct_token_id != flat_preds.predictions[seq_index][token_index][0] \\\n",
    "                    or correct_token_id != tax_preds.predictions[seq_index][token_index][0]:\n",
    "                incorrect.append({\n",
    "                    'sentence': seq_index,\n",
    "                    'morpheme': dataset['dev'][seq_index]['morphemes'][token_index],\n",
    "                    'correct_token': glosses[correct_token_id],\n",
    "                    'flat_preds': [glosses[token_id] for token_id in flat_preds.predictions[seq_index][token_index]],\n",
    "                    'tax_preds': [glosses[token_id] for token_id in tax_preds.predictions[seq_index][token_index]]\n",
    "                })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>morpheme</th>\n",
       "      <th>correct_token</th>\n",
       "      <th>flat_preds</th>\n",
       "      <th>tax_preds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>46</td>\n",
       "      <td>w</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, [SEP], PART, INC, PRON, ENF]</td>\n",
       "      <td>[E3S, PART, E1S, SC, E2S, E1P]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>59</td>\n",
       "      <td>w</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, [SEP], PART, PRON, INC, ENF]</td>\n",
       "      <td>[PART, E3S, E1S, SC, E2S, DEM]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>59</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, INC, PRON, [SEP], E1P]</td>\n",
       "      <td>[E3S, E1S, E1P, A2S, E2S, A1S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>67</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, PRON, INC, E1P, E2S]</td>\n",
       "      <td>[E3S, E1S, A2S, E1P, A1S, E2S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>115</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, INC, PRON, E2S, E1P]</td>\n",
       "      <td>[E3S, E1S, E1P, A2S, E2S, SC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>133</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, PRON, INC, [SEP], VT]</td>\n",
       "      <td>[E3S, E1S, E1P, A2S, E2S, A1S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>157</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, INC, PRON, VT, [SEP]]</td>\n",
       "      <td>[E3S, E1S, A2S, E1P, E2S, A1S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>173</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, PRON, INC, VT, E1P]</td>\n",
       "      <td>[E3S, E1S, A2S, E1P, E2S, A1S]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>173</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, PRON, INC, VT, [SEP]]</td>\n",
       "      <td>[E3S, E1S, E1P, A2S, E2S, SC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>177</td>\n",
       "      <td>in</td>\n",
       "      <td>E1S</td>\n",
       "      <td>[E1S, A1S, INC, PRON, [SEP], VT]</td>\n",
       "      <td>[E3S, E1S, E1P, SC, A2S, E2S]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sentence morpheme correct_token                          flat_preds   \n",
       "90         46        w           E1S  [E1S, [SEP], PART, INC, PRON, ENF]  \\\n",
       "113        59        w           E1S  [E1S, [SEP], PART, PRON, INC, ENF]   \n",
       "114        59       in           E1S   [E1S, A1S, INC, PRON, [SEP], E1P]   \n",
       "137        67       in           E1S     [E1S, A1S, PRON, INC, E1P, E2S]   \n",
       "213       115       in           E1S     [E1S, A1S, INC, PRON, E2S, E1P]   \n",
       "252       133       in           E1S    [E1S, A1S, PRON, INC, [SEP], VT]   \n",
       "319       157       in           E1S    [E1S, A1S, INC, PRON, VT, [SEP]]   \n",
       "350       173       in           E1S      [E1S, A1S, PRON, INC, VT, E1P]   \n",
       "351       173       in           E1S    [E1S, A1S, PRON, INC, VT, [SEP]]   \n",
       "355       177       in           E1S    [E1S, A1S, INC, PRON, [SEP], VT]   \n",
       "\n",
       "                          tax_preds  \n",
       "90   [E3S, PART, E1S, SC, E2S, E1P]  \n",
       "113  [PART, E3S, E1S, SC, E2S, DEM]  \n",
       "114  [E3S, E1S, E1P, A2S, E2S, A1S]  \n",
       "137  [E3S, E1S, A2S, E1P, A1S, E2S]  \n",
       "213   [E3S, E1S, E1P, A2S, E2S, SC]  \n",
       "252  [E3S, E1S, E1P, A2S, E2S, A1S]  \n",
       "319  [E3S, E1S, A2S, E1P, E2S, A1S]  \n",
       "350  [E3S, E1S, A2S, E1P, E2S, A1S]  \n",
       "351   [E3S, E1S, E1P, A2S, E2S, SC]  \n",
       "355   [E3S, E1S, E1P, SC, A2S, E2S]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect = pd.DataFrame(incorrect)\n",
    "incorrect[incorrect['correct_token'] == 'E1S']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
