{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3e3a82-ad41-4371-92e6-cdf1ea241722",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "02512dc8-f550-4338-ae06-9be10ea3cf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_handling import write_igt, load_data_file\n",
    "import random\n",
    "\n",
    "story = load_data_file(\"../data/GenBench/categories/story\")\n",
    "advice = load_data_file(\"../data/GenBench/categories/advice\")\n",
    "history = load_data_file(\"../data/GenBench/categories/history\")\n",
    "personal = load_data_file(\"../data/GenBench/categories/personal\")\n",
    "\n",
    "id_data = story + history\n",
    "ood_data = advice + personal\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(id_data)\n",
    "random.shuffle(ood_data)\n",
    "\n",
    "count_ood = int(len(ood_data) / 2)\n",
    "\n",
    "eval_ood = ood_data[:count_ood]\n",
    "test_ood = ood_data[count_ood:]\n",
    "\n",
    "eval_id = id_data[:count_ood]\n",
    "train = id_data[count_ood:]\n",
    "\n",
    "write_igt(eval_ood, '../data/GenBench/eval_ood.txt')\n",
    "write_igt(eval_id, '../data/GenBench/eval_id.txt')\n",
    "write_igt(test_ood, '../data/GenBench/test_ood.txt')\n",
    "write_igt(train, '../data/GenBench/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d7591b2c-88a1-41bd-90d2-4bc277476f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3f83e065f4c4307b7116d679e672dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5049 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef0080dd0d34458a869d8520d61e5bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2893c81edd684a0487f73e99e59890e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88a5f7d507324e969c836475fd15d841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_handling import create_vocab, prepare_dataset, create_gloss_vocab\n",
    "from uspanteko_morphology import morphology\n",
    "from tokenizer import WordLevelTokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "MODEL_INPUT_LENGTH = 64\n",
    "device = 'mps'\n",
    "\n",
    "train_vocab = create_vocab([line.morphemes() for line in train], threshold=1)\n",
    "tokenizer = WordLevelTokenizer(vocab=train_vocab, model_max_length=MODEL_INPUT_LENGTH)\n",
    "glosses = create_gloss_vocab(morphology)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = prepare_dataset(data=train, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['dev'] = prepare_dataset(data=eval_id, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['dev_OOD'] = prepare_dataset(data=eval_ood, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['test'] = prepare_dataset(data=test_ood, tokenizer=tokenizer, labels=glosses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d6e07668-ef95-4745-8d89-e8010eb53b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/usp-mlm-absolute-micro/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/usp-mlm-absolute-micro\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2291\n",
      "}\n",
      "\n",
      "loading weights file ../models/usp-mlm-absolute-micro/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ../models/usp-mlm-absolute-micro.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: segmentation, transcription, glosses, translation, morphemes. If segmentation, transcription, glosses, translation, morphemes are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: segmentation, transcription, glosses, translation, morphemes. If segmentation, transcription, glosses, translation, morphemes are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (id): 77.78\n",
      "Perplexity (ood): 94.03\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForMaskedLM\n",
    "import math\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"../models/usp-mlm-absolute-micro\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"pt\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=EPOCHS,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['dev'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate(dataset['dev'])\n",
    "print(f\"Perplexity (id): {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "eval_results = trainer.evaluate(dataset['dev_OOD'])\n",
    "print(f\"Perplexity (ood): {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "1693f03c-4ff3-4f0f-bd09-de0a4c062fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/full-flat-1-0.75wd/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/full-flat-1-0.75wd\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2291\n",
      "}\n",
      "\n",
      "loading weights file ../models/full-flat-1-0.75wd/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ../models/full-flat-1-0.75wd.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: segmentation, transcription, glosses, translation, morphemes. If segmentation, transcription, glosses, translation, morphemes are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[61  1 46 ... 43 43 43]\n",
      " [54  1 25 ... 43 43 43]\n",
      " [38  1 53 ... 43 43 43]\n",
      " ...\n",
      " [30 65  1 ... 43 43 43]\n",
      " [38  1 42 ... 43 43 43]\n",
      " [61  1 30 ... 43 43 43]]\n",
      "LABELS [[  61    1   46 ... -100 -100 -100]\n",
      " [  54    1   25 ... -100 -100 -100]\n",
      " [  38    1   53 ... -100 -100 -100]\n",
      " ...\n",
      " [  30   32    1 ... -100 -100 -100]\n",
      " [  38    1   42 ... -100 -100 -100]\n",
      " [  61    1   30 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['CONJ', '[SEP]', 'VOC', '[SEP]', 'CONJ', '[SEP]', 'VOC']\n",
      "Labels:\t ['CONJ', '[SEP]', 'VOC', '[SEP]', 'CONJ', '[SEP]', 'VOC']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6546702980995178,\n",
       " 'eval_average_accuracy': 0.8352711496406285,\n",
       " 'eval_accuracy': 0.8458185340652244,\n",
       " 'eval_runtime': 3.3492,\n",
       " 'eval_samples_per_second': 635.382,\n",
       " 'eval_steps_per_second': 10.152}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from finetune_token_classifier import create_trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"../models/full-flat-1-0.75wd\")\n",
    "\n",
    "trainer = create_trainer(model, dataset=dataset, tokenizer=tokenizer, labels=glosses, batch_size=BATCH_SIZE,\n",
    "                         max_epochs=300, weight_decay=0, report_to='wandb')\n",
    "\n",
    "trainer.evaluate(dataset[\"dev\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "34f02f04-fe26-4fee-8a9d-b3b97801363a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: segmentation, transcription, glosses, translation, morphemes. If segmentation, transcription, glosses, translation, morphemes are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[56  1 65 ... 43 43 43]\n",
      " [54  1 26 ... 43 43 43]\n",
      " [39  1 25 ... 43 43 43]\n",
      " ...\n",
      " [26 38 14 ... 43 43 43]\n",
      " [25 39  1 ... 43 43 43]\n",
      " [41  1 26 ... 43 43 43]]\n",
      "LABELS [[  56    1   65 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]\n",
      " [  39    1   25 ... -100 -100 -100]\n",
      " ...\n",
      " [  26   39   34 ... -100 -100 -100]\n",
      " [  25   39    1 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['NEG', '[SEP]', 'PREP']\n",
      "Labels:\t ['NEG', '[SEP]', 'PREP']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.9338552355766296,\n",
       " 'eval_average_accuracy': 0.7497611989743314,\n",
       " 'eval_accuracy': 0.7505427702996093,\n",
       " 'eval_runtime': 3.3213,\n",
       " 'eval_samples_per_second': 640.717,\n",
       " 'eval_steps_per_second': 10.237}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"dev_OOD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7a9da506-5a8b-49d7-939c-b9117d167361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: segmentation, transcription, glosses, translation, morphemes. If segmentation, transcription, glosses, translation, morphemes are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[56  1 65 ... 43 43 43]\n",
      " [54  1 26 ... 43 43 43]\n",
      " [39  1 25 ... 43 43 43]\n",
      " ...\n",
      " [26 38 14 ... 43 43 43]\n",
      " [25 39  1 ... 43 43 43]\n",
      " [41  1 26 ... 43 43 43]]\n",
      "LABELS [[  56    1   65 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]\n",
      " [  39    1   25 ... -100 -100 -100]\n",
      " ...\n",
      " [  26   39   34 ... -100 -100 -100]\n",
      " [  25   39    1 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['NEG', '[SEP]', 'PREP']\n",
      "Labels:\t ['NEG', '[SEP]', 'PREP']\n",
      "1322 total unknown tokens, 854 wrong = 0.6459909228441755\n",
      "12496 total known tokens, 2593 wrong = 0.20750640204865556\n",
      "3447 total wrong of 13818 total tokens\n"
     ]
    }
   ],
   "source": [
    "unknown_tokens = 0\n",
    "unknown_token_wrong = 0\n",
    "known_tokens = 0\n",
    "known_token_wrong = 0\n",
    "\n",
    "preds = trainer.predict(dataset[\"dev_OOD\"])\n",
    "\n",
    "incorrect_unknown_rows = []\n",
    "\n",
    "for row_index, row in enumerate(dataset[\"dev_OOD\"]):\n",
    "    for pos in range(len(row['input_ids'])):\n",
    "        if row['input_ids'][pos] == 2:\n",
    "            break\n",
    "        if row['input_ids'][pos] == 1:\n",
    "            continue\n",
    "        \n",
    "        if row['input_ids'][pos] == 0:\n",
    "            unknown_tokens += 1\n",
    "            if preds.predictions[row_index][pos] != row['labels'][pos]:\n",
    "                # Incorrect\n",
    "                incorrect_unknown_rows.append(row_index)\n",
    "                unknown_token_wrong += 1\n",
    "        else:\n",
    "            known_tokens += 1\n",
    "            if preds.predictions[row_index][pos] != row['labels'][pos]:\n",
    "                # Incorrect\n",
    "                known_token_wrong += 1\n",
    "                \n",
    "print(f\"{unknown_tokens} total unknown tokens, {unknown_token_wrong} wrong = {unknown_token_wrong / unknown_tokens}\")\n",
    "print(f\"{known_tokens} total known tokens, {known_token_wrong} wrong = {known_token_wrong / known_tokens}\")\n",
    "print(f\"{unknown_token_wrong + known_token_wrong} total wrong of {known_tokens + unknown_tokens} total tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "04bffa59-8b4a-4b95-99c2-312a791c1eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/usp-gloss-denoiser-full/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/usp-gloss-denoiser-full\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70\n",
      "}\n",
      "\n",
      "loading weights file ../models/usp-gloss-denoiser-full/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ../models/usp-gloss-denoiser-full.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb11513b24c4d6e9f6e5e72c9ff33fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\t\t [576, 1, 0, 1, 2132, 1, 1631, 0, 1, 1841, 1631, 198, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Labels\t\t [61, 1, 54, 1, 36, 1, 9, 43, 1, 26, 9, 39, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Initial Preds\t tensor([[61,  1, 41,  1, 36,  1,  9, 43,  1, 26,  9, 39, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43]])\n",
      "Processed Preds\t tensor([[65,  1,  3,  1, 40,  1, 13,  3,  1, 30, 13, 43, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Preds\t\t tensor([[65,  1, 58,  1, 40,  1, 13, 47,  1, 30, 13, 43, 38, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Input\t\t [1507, 1, 1425, 1, 1916, 1, 1425, 1, 1702, 0, 1, 1138, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Labels\t\t [54, 1, 42, 1, 27, 1, 42, 1, 40, 44, 1, 65, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Initial Preds\t tensor([[54,  1, 42,  1, 27,  1, 42,  1, 43, 43,  1, 60, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43]])\n",
      "Processed Preds\t tensor([[58,  1, 46,  1, 31,  1, 46,  1, 47,  3,  1, 64, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Preds\t\t tensor([[58,  1, 46,  1, 60,  1, 46,  1, 47, 66,  1, 64, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Input\t\t [1677, 1, 1138, 1, 0, 1, 964, 577, 1, 662, 411, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Labels\t\t [42, 1, 60, 1, 64, 1, 43, 62, 1, 14, 43, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Initial Preds\t tensor([[42,  1, 60,  1, 43,  1, 43, 62,  1, 14, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43]])\n",
      "Processed Preds\t tensor([[46,  1, 64,  1,  3,  1, 47, 66,  1, 18, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Preds\t\t tensor([[46,  1, 64,  1, 60,  1, 47, 66,  1, 18, 47, 57, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Input\t\t [846, 1, 1640, 1, 1138, 1, 2117, 1, 298, 2055, 1, 665, 1, 1657, 813, 1, 964, 1, 2189, 1, 1409, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "Labels\t\t [54, 1, 57, 1, 60, 1, 28, 1, 65, 58, 1, 43, 1, 14, 58, 1, 43, 1, 38, 1, 38, 57, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "Initial Preds\t tensor([[54,  1, 57,  1, 60,  1, 58,  1, 65, 58,  1, 43,  1, 14, 58,  1, 43,  1,\n",
      "         38,  1, 38, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43]])\n",
      "Processed Preds\t tensor([[58,  1, 61,  1, 64,  1, 62,  1, 69, 62,  1, 47,  1, 18, 62,  1, 47,  1,\n",
      "         42,  1, 42,  3, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "Preds\t\t tensor([[58,  1, 61,  1, 64,  1, 62,  1, 69, 62,  1, 47,  1, 18, 62,  1, 47,  1,\n",
      "         42,  1, 42, 36, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "2/5\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "total_fixed = 0\n",
    "total = 0\n",
    "\n",
    "denoiser = AutoModelForMaskedLM.from_pretrained(\"../models/usp-gloss-denoiser-full\")\n",
    "\n",
    "for row_id in tqdm(incorrect_unknown_rows[:4]):\n",
    "    test_row = dataset['dev_OOD'][row_id]\n",
    "    test_preds = torch.LongTensor([preds.predictions[row_id]])\n",
    "    print(\"Input\\t\\t\", test_row['input_ids'])\n",
    "    print(\"Labels\\t\\t\", test_row['labels'])\n",
    "    print(\"Initial Preds\\t\", test_preds)\n",
    "\n",
    "    test_preds[test_preds != 1] = test_preds[test_preds != 1] + 4\n",
    "\n",
    "    input_ids = torch.LongTensor([test_row['input_ids']])\n",
    "    test_preds[input_ids == 0] = 3 # MASK unknown word\n",
    "    test_preds = test_preds.narrow(-1, 0, 60)\n",
    "\n",
    "    print(\"Processed Preds\\t\", test_preds)\n",
    "\n",
    "    attention_mask = torch.LongTensor([test_row['attention_mask']])\n",
    "    attention_mask = attention_mask.narrow(-1, 0, 60)\n",
    "\n",
    "    denoised_preds = denoiser.forward(input_ids=test_preds, attention_mask=attention_mask).logits.argmax(dim=2)\n",
    "    print(\"Preds\\t\\t\", denoised_preds)\n",
    "\n",
    "    correct = denoised_preds[input_ids.narrow(-1, 0, 60) == 0] - 4 == torch.LongTensor([test_row['labels']])[input_ids == 0]\n",
    "    correct = correct.long()\n",
    "    total += torch.numel(correct)\n",
    "    total_fixed += torch.sum(correct)\n",
    "    \n",
    "print(f\"{total_fixed}/{total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "780ca1ca-19e0-4cd6-8d54-9a604a98b819",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': 'I per parej tqatij jino awe aat,', 'translation': 'Pero lo tomemos parejo, tanto para usted como para mi.', 'glosses': ['CONJ', '[SEP]', 'ADV', '[SEP]', 'ADV', '[SEP]', 'INC', 'E1P', 'VT', '[SEP]', 'AFI', '[SEP]', 'E2S', 'SREL', '[SEP]', 'PRON'], 'segmentation': 'i per parej t-qa-tij jino aw-e aat', 'morphemes': ['i', '[SEP]', 'per', '[SEP]', 'parej', '[SEP]', 't', 'qa', 'tij', '[SEP]', 'jino', '[SEP]', 'aw', 'e', '[SEP]', 'aat'], 'input_ids': [576, 1, 1514, 1, 1481, 1, 1841, 1631, 1913, 1, 707, 1, 170, 463, 1, 59, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [61, 1, 54, 1, 54, 1, 26, 9, 39, 1, 55, 1, 13, 58, 1, 42, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n",
      "tensor([[41,  1, 26, 38,  1, 54,  1, 57, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43,\n",
      "         43, 43, 43, 43, 43, 43, 43, 43, 43, 43]])\n",
      "tensor([[45,  1, 30, 42,  1, 58,  1, 61, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
      "         47, 47, 47, 47, 47, 47]])\n",
      "tensor([[[-1.4065,  1.9758, -1.6510,  ..., -0.8489, -1.5732,  0.2123],\n",
      "         [-1.9598,  5.3125, -2.1422,  ..., -1.9185, -1.8768, -0.5044],\n",
      "         [-1.7846,  3.1631, -1.8998,  ..., -1.5050, -1.7054,  0.0937],\n",
      "         ...,\n",
      "         [-1.9688,  4.3131, -2.1813,  ..., -1.8975, -1.8686, -0.0531],\n",
      "         [-1.8499,  4.4746, -2.0849,  ..., -1.8366, -1.8706, -0.2679],\n",
      "         [-1.7449,  4.3452, -1.9716,  ..., -1.7846, -1.7567, -0.2171]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "test_row = dataset['train'][1]\n",
    "test_preds = torch.LongTensor([preds.predictions[row_id]])\n",
    "print(test_row)\n",
    "print(test_preds)\n",
    "\n",
    "test_preds[test_preds != 1] = test_preds[test_preds != 1] + 4\n",
    "\n",
    "input_ids = torch.LongTensor([test_row['input_ids']])\n",
    "# test_preds[input_ids == 0] = 3 # MASK unknown word\n",
    "test_preds = test_preds.narrow(-1, 0, 60)\n",
    "\n",
    "print(test_preds)\n",
    "\n",
    "attention_mask = torch.LongTensor([test_row['attention_mask']])\n",
    "attention_mask = attention_mask.narrow(-1, 0, 60)\n",
    "\n",
    "denoised_preds = denoiser.forward(input_ids=test_preds, attention_mask=attention_mask).logits.argmax(dim=2)\n",
    "\n",
    "print(denoised_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b11b4aae-d1ac-49d4-acb6-79d889ef6ec8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25bf26ae332b4e12890543c33ea32e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5049 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5747c46dbec246b8aa97f63439e12b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_handling import prepare_dataset_mlm\n",
    "\n",
    "\n",
    "print(\"Preparing datasets...\")\n",
    "\n",
    "glosses = create_gloss_vocab(morphology)\n",
    "tokenizer = WordLevelTokenizer(vocab=glosses, model_max_length=MODEL_INPUT_LENGTH)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = prepare_dataset_mlm(data=[line.gloss_list(segmented=True) for line in train],\n",
    "                                       tokenizer=tokenizer,\n",
    "                                       device=device)\n",
    "dataset['dev'] = prepare_dataset_mlm(data=[line.gloss_list(segmented=True) for line in eval_id],\n",
    "                                     tokenizer=tokenizer,\n",
    "                                     device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d9e463ee-ce4c-4100-8582-2bdf5657dfe7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['CONJ',\n",
       "  '[SEP]',\n",
       "  'ADV',\n",
       "  '[SEP]',\n",
       "  'ADV',\n",
       "  '[SEP]',\n",
       "  'INC',\n",
       "  'E1P',\n",
       "  'VT',\n",
       "  '[SEP]',\n",
       "  'AFI',\n",
       "  '[SEP]',\n",
       "  'E2S',\n",
       "  'SREL',\n",
       "  '[SEP]',\n",
       "  'PRON'],\n",
       " 'input_ids': [65, 1, 58, 1, 58, 1, 30, 13, 43, 1, 59, 1, 17, 62, 1, 46]}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "d85731e1-65db-40e8-8d87-3d09dd758c40",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[65,  1, 58,  1, 40,  1, 13, 47,  1, 30, 13, 43]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# denoiser = AutoModelForMaskedLM.from_pretrained(\"../models/usp-gloss-denoiser-full\")\n",
    "\n",
    "denoiser.forward(input_ids=torch.LongTensor([[65,  1,  3,  1, 40,  1, 13,  3,  1, 30, 13, 43]])).logits.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "dfdc8e63-7c40-4cea-80f7-6e98d8388c6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glosses[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88aa5ba3-1201-4b82-9986-ba60197c7a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
