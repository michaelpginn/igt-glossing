{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d3e3a82-ad41-4371-92e6-cdf1ea241722",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02512dc8-f550-4338-ae06-9be10ea3cf4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from data_handling import write_igt, load_data_file\n",
    "import random\n",
    "\n",
    "story = load_data_file(\"../data/GenBench/categories/story\")\n",
    "advice = load_data_file(\"../data/GenBench/categories/advice\")\n",
    "history = load_data_file(\"../data/GenBench/categories/history\")\n",
    "personal = load_data_file(\"../data/GenBench/categories/personal\")\n",
    "\n",
    "id_data = story + history\n",
    "ood_data = advice + personal\n",
    "\n",
    "random.seed(1)\n",
    "random.shuffle(id_data)\n",
    "random.shuffle(ood_data)\n",
    "\n",
    "count_ood = int(len(ood_data) / 2)\n",
    "\n",
    "eval_ood = ood_data[:count_ood]\n",
    "test_ood = ood_data[count_ood:]\n",
    "\n",
    "eval_id = id_data[:count_ood]\n",
    "train = id_data[count_ood:]\n",
    "\n",
    "write_igt(eval_ood, '../data/GenBench/eval_ood.txt')\n",
    "write_igt(eval_id, '../data/GenBench/eval_id.txt')\n",
    "write_igt(test_ood, '../data/GenBench/test_ood.txt')\n",
    "write_igt(train, '../data/GenBench/train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7591b2c-88a1-41bd-90d2-4bc277476f87",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c53614816346ff8981997ea33ce439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5049 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be79fa4cf32f41e2909a9bb22a0264c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeaa26887b8c4f45a296c1237bd23b82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3591c668ca5648e8a09e49b9cb0d549b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from data_handling import create_vocab, prepare_dataset, create_gloss_vocab\n",
    "from uspanteko_morphology import morphology\n",
    "from tokenizer import WordLevelTokenizer\n",
    "from datasets import DatasetDict\n",
    "\n",
    "MODEL_INPUT_LENGTH = 64\n",
    "device = 'mps'\n",
    "\n",
    "train_vocab = create_vocab([line.morphemes() for line in train], threshold=1)\n",
    "tokenizer = WordLevelTokenizer(vocab=train_vocab, model_max_length=MODEL_INPUT_LENGTH)\n",
    "glosses = create_gloss_vocab(morphology)\n",
    "\n",
    "dataset = DatasetDict()\n",
    "dataset['train'] = prepare_dataset(data=train, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['dev'] = prepare_dataset(data=eval_id, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['dev_OOD'] = prepare_dataset(data=eval_ood, tokenizer=tokenizer, labels=glosses, device=device)\n",
    "dataset['test'] = prepare_dataset(data=test_ood, tokenizer=tokenizer, labels=glosses, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6e07668-ef95-4745-8d89-e8010eb53b11",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='34' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [34/34 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichael-ginn\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/milesper/Documents/Research/TaxoMorph/src/wandb/run-20230830_220246-2m5eogqd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/michael-ginn/huggingface/runs/2m5eogqd\" target=\"_blank\">../training-checkpoints</a></strong> to <a href=\"https://wandb.ai/michael-ginn/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForMaskedLM.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (id): 77.78\n",
      "Perplexity (ood): 94.03\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling, AutoModelForMaskedLM\n",
    "import math\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"../models/usp-mlm-absolute-micro\")\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"pt\")\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"../training-checkpoints\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=3,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=EPOCHS,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['dev'],\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "\n",
    "eval_results = trainer.evaluate(dataset['dev'])\n",
    "print(f\"Perplexity (id): {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "eval_results = trainer.evaluate(dataset['dev_OOD'])\n",
    "print(f\"Perplexity (ood): {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1693f03c-4ff3-4f0f-bd09-de0a4c062fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/full-flat-1-finetune-0.0wd-0.25itps-it1-test/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/full-flat-1-finetune-0.0wd-0.25itps-it1-test\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForTokenClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\",\n",
      "    \"39\": \"LABEL_39\",\n",
      "    \"40\": \"LABEL_40\",\n",
      "    \"41\": \"LABEL_41\",\n",
      "    \"42\": \"LABEL_42\",\n",
      "    \"43\": \"LABEL_43\",\n",
      "    \"44\": \"LABEL_44\",\n",
      "    \"45\": \"LABEL_45\",\n",
      "    \"46\": \"LABEL_46\",\n",
      "    \"47\": \"LABEL_47\",\n",
      "    \"48\": \"LABEL_48\",\n",
      "    \"49\": \"LABEL_49\",\n",
      "    \"50\": \"LABEL_50\",\n",
      "    \"51\": \"LABEL_51\",\n",
      "    \"52\": \"LABEL_52\",\n",
      "    \"53\": \"LABEL_53\",\n",
      "    \"54\": \"LABEL_54\",\n",
      "    \"55\": \"LABEL_55\",\n",
      "    \"56\": \"LABEL_56\",\n",
      "    \"57\": \"LABEL_57\",\n",
      "    \"58\": \"LABEL_58\",\n",
      "    \"59\": \"LABEL_59\",\n",
      "    \"60\": \"LABEL_60\",\n",
      "    \"61\": \"LABEL_61\",\n",
      "    \"62\": \"LABEL_62\",\n",
      "    \"63\": \"LABEL_63\",\n",
      "    \"64\": \"LABEL_64\",\n",
      "    \"65\": \"LABEL_65\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_39\": 39,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_40\": 40,\n",
      "    \"LABEL_41\": 41,\n",
      "    \"LABEL_42\": 42,\n",
      "    \"LABEL_43\": 43,\n",
      "    \"LABEL_44\": 44,\n",
      "    \"LABEL_45\": 45,\n",
      "    \"LABEL_46\": 46,\n",
      "    \"LABEL_47\": 47,\n",
      "    \"LABEL_48\": 48,\n",
      "    \"LABEL_49\": 49,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_50\": 50,\n",
      "    \"LABEL_51\": 51,\n",
      "    \"LABEL_52\": 52,\n",
      "    \"LABEL_53\": 53,\n",
      "    \"LABEL_54\": 54,\n",
      "    \"LABEL_55\": 55,\n",
      "    \"LABEL_56\": 56,\n",
      "    \"LABEL_57\": 57,\n",
      "    \"LABEL_58\": 58,\n",
      "    \"LABEL_59\": 59,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_60\": 60,\n",
      "    \"LABEL_61\": 61,\n",
      "    \"LABEL_62\": 62,\n",
      "    \"LABEL_63\": 63,\n",
      "    \"LABEL_64\": 64,\n",
      "    \"LABEL_65\": 65,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 2291\n",
      "}\n",
      "\n",
      "loading weights file ../models/full-flat-1-finetune-0.0wd-0.25itps-it1-test/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForTokenClassification.\n",
      "\n",
      "All the weights of RobertaForTokenClassification were initialized from the model checkpoint at ../models/full-flat-1-finetune-0.0wd-0.25itps-it1-test.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForTokenClassification for predictions without further training.\n",
      "PyTorch: setting up devices\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating trainer...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[46  1 54 ... 39 39 39]\n",
      " [54  1 56 ... 39 39 39]\n",
      " [54 62  1 ... 43 43 43]\n",
      " ...\n",
      " [38 43 43 ... 43 43 43]\n",
      " [39 32  1 ... 43 43 43]\n",
      " [26 38  1 ... 39 39 39]]\n",
      "LABELS [[  46    1   54 ... -100 -100 -100]\n",
      " [  54    1   57 ... -100 -100 -100]\n",
      " [  54   34    1 ... -100 -100 -100]\n",
      " ...\n",
      " [  38 -100 -100 ... -100 -100 -100]\n",
      " [  38   61    1 ... -100 -100 -100]\n",
      " [  26   39    1 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n",
      "Labels:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.8489748239517212,\n",
       " 'eval_average_accuracy': 0.7579537200672487,\n",
       " 'eval_accuracy': 0.7614341364882412,\n",
       " 'eval_runtime': 3.2319,\n",
       " 'eval_samples_per_second': 658.431,\n",
       " 'eval_steps_per_second': 10.52}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "from finetune_token_classifier import create_trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"../models/full-flat-1-finetune-0.0wd-0.25itps-it1-test\")\n",
    "\n",
    "trainer = create_trainer(model, dataset=dataset, tokenizer=tokenizer, labels=glosses, batch_size=BATCH_SIZE,\n",
    "                         max_epochs=300, weight_decay=0, report_to='wandb')\n",
    "\n",
    "trainer.evaluate(dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "34f02f04-fe26-4fee-8a9d-b3b97801363a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[46  1 54 ... 39 39 39]\n",
      " [54  1 56 ... 39 39 39]\n",
      " [54 62  1 ... 39 39 39]\n",
      " ...\n",
      " [38 43 43 ... 43 43 43]\n",
      " [39 32  1 ... 43 43 43]\n",
      " [26 38  1 ... 39 39 39]]\n",
      "LABELS [[  46    1   54 ... -100 -100 -100]\n",
      " [  54    1   57 ... -100 -100 -100]\n",
      " [  54   34    1 ... -100 -100 -100]\n",
      " ...\n",
      " [  38 -100 -100 ... -100 -100 -100]\n",
      " [  38   61    1 ... -100 -100 -100]\n",
      " [  26   39    1 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n",
      "Labels:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.7491924166679382,\n",
       " 'eval_average_accuracy': 0.7587232684322329,\n",
       " 'eval_accuracy': 0.761578415813014,\n",
       " 'eval_runtime': 3.2315,\n",
       " 'eval_samples_per_second': 658.527,\n",
       " 'eval_steps_per_second': 10.522}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(dataset[\"dev_OOD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a9da506-5a8b-49d7-939c-b9117d167361",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "967c4b03c4fc4c1c8a4c5d3e34ab601b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2128 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[56  1 65 ... 43 43 43]\n",
      " [54  1 26 ... 39 39 39]\n",
      " [39  1 25 ... 43 43 43]\n",
      " ...\n",
      " [26 38 14 ... 43 43 43]\n",
      " [25 39  1 ... 39 39 39]\n",
      " [41  1 26 ... 39 39 39]]\n",
      "LABELS [[  56    1   65 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]\n",
      " [  39    1   25 ... -100 -100 -100]\n",
      " ...\n",
      " [  26   39   34 ... -100 -100 -100]\n",
      " [  25   39    1 ... -100 -100 -100]\n",
      " [  54    1   26 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['NEG', '[SEP]', 'PREP']\n",
      "Labels:\t ['NEG', '[SEP]', 'PREP']\n"
     ]
    }
   ],
   "source": [
    "# Write predictions\n",
    "from data_handling import write_predictions\n",
    "\n",
    "write_predictions(eval_ood, tokenizer, trainer, glosses, '../data/GenBench/pred_eval_ood_0.25_it2.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9614e4f0-69aa-4fd5-8753-ef32eb54e417",
   "metadata": {},
   "source": [
    "## Denoiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04bffa59-8b4a-4b95-99c2-312a791c1eae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 2128\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[46  1 54 ... 39 39 39]\n",
      " [54  1 56 ... 39 39 39]\n",
      " [54 62  1 ... 43 43 43]\n",
      " ...\n",
      " [38 43 43 ... 43 43 43]\n",
      " [39 32  1 ... 43 43 43]\n",
      " [26 38  1 ... 39 39 39]]\n",
      "LABELS [[  46    1   54 ... -100 -100 -100]\n",
      " [  54    1   57 ... -100 -100 -100]\n",
      " [  54   34    1 ... -100 -100 -100]\n",
      " ...\n",
      " [  38 -100 -100 ... -100 -100 -100]\n",
      " [  38   61    1 ... -100 -100 -100]\n",
      " [  26   39    1 ... -100 -100 -100]]\n",
      "(2128, 64)\n",
      "Preds:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n",
      "Labels:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ../models/usp-gloss-denoiser-micro/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"../models/usp-gloss-denoiser-micro\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 100,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 64,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 5,\n",
      "  \"num_hidden_layers\": 3,\n",
      "  \"pad_token_id\": 2,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 70\n",
      "}\n",
      "\n",
      "loading weights file ../models/usp-gloss-denoiser-micro/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327 total unknown tokens, 1053 wrong = 0.7935192162773173\n",
      "12535 total known tokens, 2254 wrong = 0.1798165137614679\n",
      "3307 total wrong of 13862 total tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint weights were used when initializing RobertaForMaskedLM.\n",
      "\n",
      "All the weights of RobertaForMaskedLM were initialized from the model checkpoint at ../models/usp-gloss-denoiser-micro.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForMaskedLM for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fe14f583e0433381811f53bff018d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 2051 token 4\n",
      "Row 2051 token 6\n",
      "Row 12 token 0\n",
      "Row 2062 token 20\n",
      "Row 25 token 6\n",
      "Row 2076 token 8\n",
      "Row 2076 token 14\n",
      "Row 2079 token 7\n",
      "Row 38 token 8\n",
      "Row 41 token 6\n",
      "Row 50 token 8\n",
      "Row 58 token 6\n",
      "Row 59 token 0\n",
      "Row 60 token 6\n",
      "Row 61 token 13\n",
      "Row 63 token 0\n",
      "Row 95 token 11\n",
      "Row 96 token 16\n",
      "Row 106 token 4\n",
      "Row 113 token 6\n",
      "Row 114 token 9\n",
      "Row 121 token 0\n",
      "Row 131 token 3\n",
      "Row 133 token 0\n",
      "Row 136 token 11\n",
      "Row 145 token 4\n",
      "Row 154 token 4\n",
      "Row 161 token 4\n",
      "Row 173 token 0\n",
      "Row 180 token 13\n",
      "Row 182 token 8\n",
      "Row 187 token 0\n",
      "Row 209 token 2\n",
      "Row 224 token 8\n",
      "Row 225 token 4\n",
      "Row 241 token 0\n",
      "Row 265 token 12\n",
      "Row 266 token 20\n",
      "Row 266 token 26\n",
      "Row 280 token 2\n",
      "Row 286 token 6\n",
      "Row 295 token 7\n",
      "Row 298 token 6\n",
      "Row 321 token 2\n",
      "Row 345 token 2\n",
      "Row 360 token 6\n",
      "Row 361 token 6\n",
      "Row 371 token 0\n",
      "Row 371 token 10\n",
      "Row 376 token 7\n",
      "Row 388 token 15\n",
      "Row 394 token 4\n",
      "Row 400 token 14\n",
      "Row 403 token 0\n",
      "Row 422 token 7\n",
      "Row 429 token 2\n",
      "Row 435 token 0\n",
      "Row 456 token 3\n",
      "Row 463 token 0\n",
      "Row 468 token 10\n",
      "Row 473 token 14\n",
      "Row 474 token 9\n",
      "Row 478 token 7\n",
      "Row 497 token 8\n",
      "Row 511 token 8\n",
      "Row 522 token 7\n",
      "Row 525 token 2\n",
      "Row 525 token 12\n",
      "Row 530 token 11\n",
      "Row 536 token 0\n",
      "Row 536 token 10\n",
      "Row 540 token 5\n",
      "Row 543 token 8\n",
      "Row 545 token 2\n",
      "Row 547 token 6\n",
      "Row 553 token 8\n",
      "Row 560 token 4\n",
      "Row 579 token 0\n",
      "Row 579 token 10\n",
      "Row 597 token 8\n",
      "Row 622 token 11\n",
      "Row 626 token 8\n",
      "Row 643 token 6\n",
      "Row 649 token 12\n",
      "Row 656 token 0\n",
      "Row 656 token 5\n",
      "Row 668 token 4\n",
      "Row 675 token 3\n",
      "Row 688 token 9\n",
      "Row 692 token 4\n",
      "Row 710 token 0\n",
      "Row 763 token 0\n",
      "Row 774 token 13\n",
      "Row 788 token 6\n",
      "Row 792 token 6\n",
      "Row 806 token 0\n",
      "Row 811 token 7\n",
      "Row 812 token 12\n",
      "Row 820 token 2\n",
      "Row 821 token 5\n",
      "Row 832 token 0\n",
      "Row 847 token 4\n",
      "Row 863 token 13\n",
      "Row 880 token 14\n",
      "Row 888 token 3\n",
      "Row 905 token 0\n",
      "Row 932 token 8\n",
      "Row 958 token 10\n",
      "Row 964 token 3\n",
      "Row 970 token 2\n",
      "Row 1010 token 8\n",
      "Row 1014 token 10\n",
      "Row 1024 token 10\n",
      "Row 1025 token 2\n",
      "Row 1038 token 2\n",
      "Row 1055 token 6\n",
      "Row 1059 token 9\n",
      "Row 1073 token 8\n",
      "Row 1096 token 4\n",
      "Row 1108 token 5\n",
      "Row 1108 token 12\n",
      "Row 1122 token 4\n",
      "Row 1124 token 4\n",
      "Row 1126 token 4\n",
      "Row 1147 token 15\n",
      "Row 1151 token 0\n",
      "Row 1151 token 7\n",
      "Row 1151 token 9\n",
      "Row 1167 token 6\n",
      "Row 1173 token 8\n",
      "Row 1174 token 8\n",
      "Row 1177 token 0\n",
      "Row 1191 token 7\n",
      "Row 1197 token 4\n",
      "Row 1199 token 2\n",
      "Row 1199 token 6\n",
      "Row 1200 token 4\n",
      "Row 1220 token 5\n",
      "Row 1224 token 0\n",
      "Row 1224 token 3\n",
      "Row 1226 token 4\n",
      "Row 1266 token 7\n",
      "Row 1267 token 5\n",
      "Row 1278 token 2\n",
      "Row 1283 token 9\n",
      "Row 1308 token 2\n",
      "Row 1310 token 0\n",
      "Row 1313 token 7\n",
      "Row 1317 token 2\n",
      "Row 1321 token 2\n",
      "Row 1324 token 4\n",
      "Row 1367 token 9\n",
      "Row 1370 token 7\n",
      "Row 1378 token 7\n",
      "Row 1381 token 9\n",
      "Row 1389 token 6\n",
      "Row 1394 token 6\n",
      "Row 1403 token 0\n",
      "Row 1420 token 12\n",
      "Row 1424 token 3\n",
      "Row 1428 token 6\n",
      "Row 1444 token 6\n",
      "Row 1446 token 6\n",
      "Row 1454 token 3\n",
      "Row 1482 token 0\n",
      "Row 1503 token 2\n",
      "Row 1527 token 4\n",
      "Row 1578 token 5\n",
      "Row 1587 token 4\n",
      "Row 1608 token 16\n",
      "Row 1622 token 0\n",
      "Row 1624 token 4\n",
      "Row 1637 token 7\n",
      "Row 1647 token 4\n",
      "Row 1651 token 6\n",
      "Row 1653 token 21\n",
      "Row 1656 token 6\n",
      "Row 1659 token 6\n",
      "Row 1671 token 10\n",
      "Row 1675 token 2\n",
      "Row 1684 token 4\n",
      "Row 1687 token 4\n",
      "Row 1691 token 8\n",
      "Row 1706 token 4\n",
      "Row 1739 token 6\n",
      "Row 1756 token 4\n",
      "Row 1766 token 9\n",
      "Row 1779 token 0\n",
      "Row 1781 token 0\n",
      "Row 1782 token 4\n",
      "Row 1783 token 7\n",
      "Row 1784 token 0\n",
      "Row 1784 token 8\n",
      "Row 1786 token 16\n",
      "Row 1786 token 20\n",
      "Row 1792 token 0\n",
      "Row 1799 token 0\n",
      "Row 1799 token 4\n",
      "Row 1801 token 2\n",
      "Row 1806 token 0\n",
      "Row 1840 token 4\n",
      "Row 1853 token 6\n",
      "Row 1861 token 4\n",
      "Row 1861 token 6\n",
      "Row 1862 token 2\n",
      "Row 1864 token 0\n",
      "Row 1883 token 6\n",
      "Row 1904 token 7\n",
      "Row 1905 token 0\n",
      "Row 1907 token 6\n",
      "Row 1936 token 4\n",
      "Row 1948 token 1\n",
      "Row 1948 token 4\n",
      "Row 1986 token 8\n",
      "Row 1996 token 0\n",
      "Row 1996 token 10\n",
      "Row 2021 token 2\n",
      "Row 2024 token 4\n",
      "Row 2025 token 4\n",
      "Row 2025 token 6\n",
      "Row 2025 token 10\n",
      "Row 2027 token 2\n",
      "Row 2033 token 3\n",
      "Row 2039 token 4\n",
      "457/1327 unknown from 274, with 233 shared\n",
      "Perf on unknown: 0.20648078372268275 before, 0.3443858327053504 after\n",
      "0.013201558216707546 improvement in accuracy.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "target_set = dataset[\"test\"]\n",
    "\n",
    "unknown_tokens = 0\n",
    "unknown_token_wrong = 0\n",
    "known_tokens = 0\n",
    "known_token_wrong = 0\n",
    "\n",
    "preds = trainer.predict(target_set)\n",
    "\n",
    "rows_with_unknowns = set()\n",
    "\n",
    "for row_index, row in enumerate(target_set):\n",
    "    for pos in range(len(row['input_ids'])):\n",
    "        if row['input_ids'][pos] == 2:\n",
    "            break\n",
    "        if row['input_ids'][pos] == 1:\n",
    "            continue\n",
    "        \n",
    "        if row['input_ids'][pos] == 0:\n",
    "            unknown_tokens += 1\n",
    "            rows_with_unknowns.add(row_index)\n",
    "\n",
    "            if preds.predictions[row_index][pos] != row['labels'][pos]:\n",
    "                # Incorrect\n",
    "                unknown_token_wrong += 1\n",
    "        else:\n",
    "            known_tokens += 1\n",
    "            if preds.predictions[row_index][pos] != row['labels'][pos]:\n",
    "                # Incorrect\n",
    "                known_token_wrong += 1\n",
    "                \n",
    "print(f\"{unknown_tokens} total unknown tokens, {unknown_token_wrong} wrong = {unknown_token_wrong / unknown_tokens}\")\n",
    "print(f\"{known_tokens} total known tokens, {known_token_wrong} wrong = {known_token_wrong / known_tokens}\")\n",
    "print(f\"{unknown_token_wrong + known_token_wrong} total wrong of {known_tokens + unknown_tokens} total tokens\")\n",
    "\n",
    "total_correct_before = 0\n",
    "total_fixed = 0\n",
    "both_correct = 0\n",
    "total = 0\n",
    "\n",
    "denoiser = AutoModelForMaskedLM.from_pretrained(\"../models/usp-gloss-denoiser-micro\")\n",
    "\n",
    "for row_id in tqdm(rows_with_unknowns):\n",
    "    test_row = target_set[row_id]\n",
    "    test_preds = torch.LongTensor([preds.predictions[row_id]])\n",
    "    input_ids = torch.LongTensor([test_row['input_ids']])\n",
    "    labels = torch.LongTensor([test_row['labels']])\n",
    "\n",
    "    test_preds[test_preds != 1] = test_preds[test_preds != 1] + 4\n",
    "\n",
    "    # test_preds[input_ids == 0] = 3 # MASK unknown word\n",
    "    test_preds = test_preds.narrow(-1, 0, 60)\n",
    "\n",
    "    attention_mask = torch.LongTensor([test_row['attention_mask']])\n",
    "    attention_mask = attention_mask.narrow(-1, 0, 60)\n",
    "\n",
    "    denoised_preds = denoiser.forward(input_ids=test_preds, attention_mask=attention_mask).logits.argmax(dim=2)\n",
    "    \n",
    "    for pos in range(len(test_row['input_ids'])):\n",
    "        if test_row['input_ids'][pos] == 2:\n",
    "            break\n",
    "        if test_row['input_ids'][pos] == 1:\n",
    "            continue\n",
    "        if test_row['input_ids'][pos] == 0:\n",
    "            total += 1\n",
    "            prior_correct = preds.predictions[row_id][pos] == test_row['labels'][pos]\n",
    "            post_correct = denoised_preds[0][pos] - 4 == test_row['labels'][pos]\n",
    "            if prior_correct:\n",
    "                total_correct_before += 1\n",
    "\n",
    "            if post_correct:\n",
    "                total_fixed += 1\n",
    "                if not prior_correct:\n",
    "                    print(f\"Row {row_id} token {pos}\")\n",
    "            if prior_correct and post_correct:\n",
    "                both_correct += 1\n",
    "\n",
    "    # correct = denoised_preds[input_ids.narrow(-1, 0, 60) == 0] - 4 == labels[input_ids == 0]\n",
    "    # correct = correct.long()\n",
    "    # total_fixed += torch.sum(correct)\n",
    "    \n",
    "print(f\"{total_fixed}/{total} unknown from {total_correct_before}, with {both_correct} shared\")\n",
    "print(f\"Perf on unknown: {total_correct_before / total} before, {total_fixed / total} after\")\n",
    "\n",
    "improved_tokens = total_fixed - total_correct_before\n",
    "acc_improvement = improved_tokens / (known_tokens + unknown_tokens)\n",
    "print(f\"{acc_improvement} improvement in accuracy.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34587dfa-c73a-45f0-9ed2-320c5508bf88",
   "metadata": {},
   "source": [
    "# Iterative Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ba68ffbf-6cca-47cc-ba75-9260a6b36afc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a901fd94f9554deeb3697fd1d3460292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/532 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 532\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[46  1 54 ... 39 39 39]\n",
      " [54 62  1 ... 39 39 39]\n",
      " [57 53  1 ... 43 43 43]\n",
      " ...\n",
      " [54  1 59 ... 43 43 43]\n",
      " [53 22  1 ... 39 39 39]\n",
      " [13 58 53 ... 39 39 39]]\n",
      "LABELS [[  46    1   54 ... -100 -100 -100]\n",
      " [  54   34    1 ... -100 -100 -100]\n",
      " [  57   33    1 ... -100 -100 -100]\n",
      " ...\n",
      " [  54    1   59 ... -100 -100 -100]\n",
      " [  39   22    1 ... -100 -100 -100]\n",
      " [  13   58   53 ... -100 -100 -100]]\n",
      "(532, 64)\n",
      "Preds:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n",
      "Labels:\t ['VOC', '[SEP]', 'ADV', '[SEP]', 'DEM']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3840f778ea54be78002c0714d4477bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/532 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set don't have a corresponding argument in `RobertaForTokenClassification.forward` and have been ignored: translation, glosses, transcription, morphemes, segmentation. If translation, glosses, transcription, morphemes, segmentation are not expected by `RobertaForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 532\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDS [[56  1 65 ... 43 43 43]\n",
      " [39  1 25 ... 39 39 39]\n",
      " [61  1 41 ... 39 39 39]\n",
      " ...\n",
      " [26 39 39 ... 39 39 39]\n",
      " [41  1 25 ... 39 39 39]\n",
      " [54  1 54 ... 43 43 43]]\n",
      "LABELS [[  56    1   65 ... -100 -100 -100]\n",
      " [  39    1   25 ... -100 -100 -100]\n",
      " [  61    1   54 ... -100 -100 -100]\n",
      " ...\n",
      " [  26    4   39 ... -100 -100 -100]\n",
      " [  46    1   25 ... -100 -100 -100]\n",
      " [  54    1   54 ... -100 -100 -100]]\n",
      "(532, 64)\n",
      "Preds:\t ['NEG', '[SEP]', 'PREP']\n",
      "Labels:\t ['NEG', '[SEP]', 'PREP']\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "target_set = dataset[\"test\"]\n",
    "target_data = test_ood\n",
    "\n",
    "logits = model.forward(input_ids=torch.LongTensor(target_set['input_ids']), attention_mask=torch.LongTensor(target_set['attention_mask'])).logits\n",
    "\n",
    "row_confidences = []\n",
    "\n",
    "# For each sequence, pick the top logit for each token and average log probs\n",
    "for index in range(len(target_set)):\n",
    "    row = target_set[index]\n",
    "    row_logits = logits[index] # 64 * 66\n",
    "    \n",
    "    num_tokens = len(row['morphemes'])\n",
    "    row_logits_sum = 0\n",
    "    \n",
    "    for token_index in range(num_tokens):\n",
    "        token_logits = row_logits[token_index]\n",
    "        token_logits = F.softmax(token_logits, dim=0)\n",
    "        max_token_logit = torch.max(token_logits).item()\n",
    "        row_logits_sum += max_token_logit\n",
    "    \n",
    "    avg_prob = row_logits_sum / num_tokens\n",
    "    row_confidences.append(avg_prob)\n",
    "    \n",
    "top_indices = sorted(range(len(row_confidences)), key=lambda x: row_confidences[x])[-int(len(row_confidences) / 4):]\n",
    "\n",
    "dataset['hiconf'] = prepare_dataset(data=[target_data[i] for i in range(len(eval_ood)) if i in top_indices], tokenizer=tokenizer, labels=glosses, device=device)\n",
    "trainer.evaluate(dataset['hiconf'])\n",
    "\n",
    "write_predictions([eval_ood[i] for i in range(len(target_data)) if i in top_indices], tokenizer, trainer, glosses, '../data/GenBench/pred_test_ood_0.25_it1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04d10d95-1a37-4814-880f-102f1f877913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transcription': \"wi' qaseboya, qiik,\", 'translation': 'Tenemos nuestras cebollas y nuestro chile.', 'glosses': ['EXS', '[SEP]', 'E1P', 'S', '[SEP]', 'E1P', 'S'], 'segmentation': \"wi' qa-seboya q-iik\", 'morphemes': [\"wi'\", '[SEP]', 'qa', 'seboya', '[SEP]', 'q', 'iik'], 'input_ids': [2132, 1, 1631, 0, 1, 1587, 597, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [36, 1, 9, 43, 1, 9, 43, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][2033])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9202b3e6-6285-478d-b102-4e68d85590a4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36,  1,  9, 41,  1,  9, 35, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39,\n",
       "       39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.predictions[2033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8124de11-e9e0-4dd0-9b8b-050cbe7d9768",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'S'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glosses[43]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fc889e-f0aa-472b-bd23-6411326ac5f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
